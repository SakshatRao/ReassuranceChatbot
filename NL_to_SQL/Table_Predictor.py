# -*- coding: utf-8 -*-
"""NL_to_SQL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AdhapecSz331NDy2Ypegc8bQGqhtNerL
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances

from collections import OrderedDict
import re
import io
import time
import pickle
import os
from sqlnet.utils import load_word_emb
import warnings
warnings.filterwarnings('ignore')
import os
import tensorflow as tf
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tag import pos_tag
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('punkt')

import benepar
benepar.download('benepar_en2')

def w2emb(embeddings_index, word):
    w2i, word_emb_val = embeddings_index
    return word_emb_val[w2i[word], :]

class TextPreprocessor:
    def __init__(self):
        self.stop_words=stopwords.words('english')
        self.lemmatizer=WordNetLemmatizer()
        self.alphabets_space='abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ '
        self.contractions_map={ 
            "ain't": "am not","aren't": "are not","can't": "cannot","can't've": "cannot have","'cause": "because","could've": "could have","couldn't": "could not",
            "couldn't've": "could not have","didn't": "did not","doesn't": "does not","don't": "do not","hadn't": "had not","hadn't've": "had not have",
            "hasn't": "has not","haven't": "have not","he'd": "he would","he'd've": "he would have","he'll": "he will","he'll've": "he will have",
            "he's": "he is","how'd": "how did","how'd'y": "how do you","how'll": "how will","how's": "how is","I'd": "I would","I'd've": "I would have",
            "I'll": "I will","I'll've": "I will have","I'm": "I am","I've": "I have","isn't": "is not","it'd": "it would","it'd've": "it would have",
            "it'll": "it will","it'll've": "it will have","it's": "it is","let's": "let us","ma'am": "madam","mayn't": "may not","might've": "might have",
            "mightn't": "might not","mightn't've": "might not have","must've": "must have","mustn't": "must not","mustn't've": "must not have",
            "needn't": "need not","needn't've": "need not have","o'clock": "of the clock","oughtn't": "ought not","oughtn't've": "ought not have",
            "shan't": "shall not","sha'n't": "shall not","shan't've": "shall not have","she'd": "she would","she'd've": "she would have","she'll": "she will",
            "she'll've": "she will have","she's": "she is","should've": "should have","shouldn't": "should not","shouldn't've": "should not have",
            "so've": "so have","so's": "so is","that'd": "that would","that'd've": "that would have","that's": "that is","there'd": "there would",
            "there'd've": "there would have","there's": "there is","they'd": "they had","they'd've": "they would have","they'll": "they will",
            "they'll've": "they will have","they're": "they are","they've": "they have","to've": "to have","wasn't": "was not","we'd": "we would",
            "we'd've": "we would have","we'll": "we will","we'll've": "we will have","we're": "we are","we've": "we have","weren't": "were not","what'll": "what will",
            "what'll've": "what will have","what're": "what are","what's": "what is","what've": "what have","when's": "when is","when've": "when have",
            "where'd": "where did","where's": "where is","where've": "where have","who'll": "who will","who'll've": "who will have","who's": "who is",
            "who've": "who have","why's": "why is","why've": "why have","will've": "will have","won't": "will not","won't've": "will not have",
            "would've": "would have","wouldn't": "would not","wouldn't've": "would not have","y'all": "you all","y'all'd": "you all would",
            "y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have","you'd": "you would","you'd've": "you would have",
            "you'll": "you will","you'll've": "you will have","you're": "you are","you've": "you have"
        }

    def process_word(self, word):
        word=word.strip()
        if(word in self.contractions_map):
            word=self.contractions_map[word]
        word=''.join([x for x in word if x in self.alphabets_space]).strip()
        if(pos_tag([word])[0][1] in ['VB','VBD','VBG','VBN','VBP','VBZ']):
            word=self.lemmatizer.lemmatize(word,'v')
        else:
            word=self.lemmatizer.lemmatize(word)
        if(word not in self.stop_words):
            return word
        else:
            return None

    def preprocess_text(self, text):
        text=text.lower().strip()
        words=text.split(' ')
        rel_words=[x for x in words if x not in self.stop_words]
        modified_text=[]
        for word in rel_words:
            modified_word=self.process_word(word)
            if(modified_word!=None):
                modified_text.append(modified_word)
        modified_text=' '.join(modified_text)
        return modified_text

class Plotter:

    def __init__(self, embeddings_index):
        self.embeddings_index = embeddings_index
        self.pca = PCA(2)
    
    def plot_cat_descr_vectors(self, cat_descr_map, find_avg_cat_vector):
        cat_descr_vector=OrderedDict()
        for cat in cat_descr_map:
            cat_descr_vector[cat]=find_avg_cat_vector(cat)
        cat_words=OrderedDict()
        cat_assigned_colors=['black','blue','red','cyan','lime','yellow','magenta','lightgray']
        cat_colors=[]
        for cat_ind,cat in enumerate(cat_descr_map.keys()):
            for word in cat_descr_map[cat]:
                if(word in self.embeddings_index[0]):
                    cat_words[word]=w2emb(self.embeddings_index, word)
                    cat_colors.append(cat_assigned_colors[cat_ind])
        vectors_items=cat_words.items()
        vectors_cat=[x[0] for x in vectors_items]
        vectors_300d=np.asarray([x[1] for x in vectors_items])
        vectors_2d=self.pca.fit_transform(vectors_300d)
        custom_lines=[Line2D([0],[0],marker='o',markerfacecolor=x,markersize=15,color='w') for x in cat_assigned_colors]
        self.plot_vectors(vectors_2d,vectors_cat,colors=cat_colors,title="Category Descripion Vectors",figsize=(15,15),
                    legend_content=[custom_lines,list(cat_descr_map.keys())])

    def plot_cat_vectors(self, cat_descr_vector):
        vectors_items=cat_descr_vector.items()
        vectors_cat=[x[0] for x in vectors_items]
        vectors_300d=np.asarray([x[1] for x in vectors_items])
        vectors_2d=self.pca.transform(vectors_300d)
        self.plot_vectors(vectors_2d,vectors_cat,title="Category Vectors")
        
    def plot_text_vectors(self, cat_descr_vector, text_vector, all_text_vector, all_text_words, processed_text):
        all_descr_vector=cat_descr_vector.copy()
        
        vectors_items=list(all_descr_vector.items())
        for ind,word in enumerate(all_text_words):
            vectors_items.append((word,all_text_vector[ind]))
        vectors_items.append(('input text',text_vector))
        
        vectors_cat=[x[0] for x in vectors_items]
        vectors_300d=np.asarray([x[1] for x in vectors_items])
        vectors_2d=self.pca.transform(vectors_300d)
        
        colors=['blue']*len(cat_descr_vector)+['red']*len(all_text_words)+['lime']
        custom_lines=[Line2D([0],[0],marker='o',markerfacecolor=x,markersize=15,color='w') for x in ['blue','red','lime']]
        self.plot_vectors(vectors_2d,vectors_cat,colors=colors,title="Processed Text: "+processed_text,figsize=(10,10),
                    legend_content=[custom_lines,['Category Vectors','Input Word Vectors','Final Input Vector']])
        
    def plot_vectors(self, vectors_2d,vectors_cat,colors='default',title="",figsize=(7,7),legend_content=None):
        if(colors=='default'):
            colors=['blue']*vectors_2d.shape[0]
        fig,ax=plt.subplots(1,1,figsize=figsize)
        ax.scatter(x=vectors_2d[:,0],y=vectors_2d[:,1],c=colors)
        for ind,cat in enumerate(vectors_cat):
            ax.annotate(cat,(vectors_2d[ind,0]+0.02,vectors_2d[ind,1]+0.02))
        ax.set_title(title,fontsize=15)
        ax.grid(True)
        if(legend_content!=None):
            ax.legend(*legend_content)
        plt.show()

class CategoryInitializer:
    
    def __init__(self, embeddings_index, preprocessor, plotter, embeddings_dim, schema_content, level2_map):
        self.embeddings_index = embeddings_index
        self.preprocessor = preprocessor
        self.plotter = plotter
        self.embeddings_dim = embeddings_dim
        self.schema_content = schema_content
        self.level2_map = level2_map

    def extract_table_info(self):
        info=self.schema_content.split('DROP TABLE IF EXISTS ')[1:]
        info=[x.split('CREATE TABLE ')[1] for x in info]
        info=[x.split('INSERT INTO')[0] for x in info]
        info=[re.findall(r'`[^`]*` ',x) for x in info]
        info=[[y[1:-2] for y in x] for x in info]
        self.table_info=dict([(x[0],x[1:]) for x in info])
        self.tables=list(self.table_info.keys())
    
    def find_avg_cat_vector(self, cat):
        descr=self.cat_descr_map[cat]
        descr_vectors=np.zeros((len(descr), self.embeddings_dim),dtype=np.float32)
        for ind,desc in enumerate(descr):
            if(desc in self.embeddings_index[0]):
                descr_vectors[ind]=w2emb(self.embeddings_index, desc)
        descr_vector=np.mean(descr_vectors,axis=0)
        return descr_vector

    def firstWord_mapping(self):
        firstWord_map={}
        for table in self.tables:
            firstWord_map[table]=table.split('_')[0]
        table_cats=[self.level2_map[firstWord_map[x]] for x in self.tables]

        self.cat_descr_map={
            'owners':[
                'owner','customer','person','apartment','gender','family', 'sold'
            ],
            'apartments':[
                'apartment','room','carpet', 'area', 'sold', 'floor', 'house', 'home', 'building', 'housing'
            ],
            'payments':[
                'payment','amount','paid', 'installment', 'due', 'money', 'finance', 'buy'
            ],
        }

        for cat in self.cat_descr_map:
            for ind in range(len(self.cat_descr_map[cat])):
                self.cat_descr_map[cat][ind]=self.preprocessor.process_word(self.cat_descr_map[cat][ind])

        cat_descr_vector={}
        for cat in self.cat_descr_map:
            cat_descr_vector[cat]=self.find_avg_cat_vector(cat)
        
        #self.plotter.plot_cat_descr_vectors(self.cat_descr_map, self.find_avg_cat_vector)
        #self.plotter.plot_cat_vectors(cat_descr_vector)

        return cat_descr_vector

class CategoryPredictor:
    def __init__(self, text, cat_descr_vector, embeddings_index, preprocessor, plotter, embeddings_dim):
        self.text = text
        self.cat_descr_vector = cat_descr_vector
        self.preprocessor = preprocessor
        self.plotter = plotter
        self.embeddings_index = embeddings_index
        self.embeddings_dim = embeddings_dim

    def softmax(self, cat_scores):
        cats=[x[0] for x in cat_scores.items()]
        scores=[x[1] for x in cat_scores.items()]
        exp_scores=np.exp(np.asarray(scores,dtype=np.float32))
        exp_sum=np.sum(exp_scores)
        final_scores=exp_scores/exp_sum*100.0
        final_cat_scores=dict(zip(cats,final_scores))
        return final_cat_scores

    def find_best_category(self, plot_text_vector=False):
        processed_text=self.preprocessor.preprocess_text(self.text)
        words=processed_text.split()
        all_text_vector=np.zeros((len(words), self.embeddings_dim),dtype=np.float32)
        all_text_words=[]
        cnt=0
        for word in words:
            if(word in self.embeddings_index[0]):
                all_text_vector[cnt]=w2emb(self.embeddings_index, word)
                all_text_words.append(word)
                cnt+=1
        text_vector=np.mean(all_text_vector[:cnt],axis=0)
        
        cat_cosine_scores=OrderedDict()
        cat_euclidean_scores=OrderedDict()
        for cat in self.cat_descr_vector:
            cat_cosine_scores[cat]=np.abs(cosine_similarity([text_vector],[self.cat_descr_vector[cat]])[0][0])
            cat_euclidean_scores[cat]=1-np.abs(euclidean_distances([text_vector],[self.cat_descr_vector[cat]])[0][0])
        final_cosine_scores=self.softmax(cat_cosine_scores)
        final_euclidean_scores=self.softmax(cat_euclidean_scores)
        if(plot_text_vector==False):
            return final_cosine_scores,final_euclidean_scores
        else:
            return final_cosine_scores,final_euclidean_scores,text_vector,all_text_vector,all_text_words,processed_text

    def predict_category(self, plot_text_vector = False):
        if(plot_text_vector == True):
            cosine_scores, euclidean_scores, text_vector, all_text_vector, all_text_words, processed_text = self.find_best_category(True)
            plotter.plot_text_vectors(self.cat_descr_vector, text_vector, all_text_vector, all_text_words, processed_text)
        else:
            cosine_scores, euclidean_scores = self.find_best_category(False)
        
        sorted_cosine_scores=sorted(cosine_scores.items(),key=lambda x:x[1],reverse=True)
        sorted_euclidean_scores=sorted(euclidean_scores.items(),key=lambda x:x[1],reverse=True)

        if(plot_text_vector==True):
            print("\nCosine Scores:")
            for ind,score in enumerate(sorted_cosine_scores):
                print("{ind}. {cat}: {score:.2f}%".format(ind=ind+1,cat=score[0],score=score[1]))

            print("\nEuclidean Scores:")
            for ind,score in enumerate(sorted_euclidean_scores):
                print("{ind}. {cat}: {score:.2f}%".format(ind=ind+1,cat=score[0],score=score[1]))
        
        return [x[0] for x in sorted_cosine_scores], [x[1] for x in sorted_cosine_scores]

class EmbeddingsLoader:
    def __init__(self, filepath):
        self.filepath = filepath

    def load_embeddings(self):
        w2i, word_emb_val = load_word_emb('glove/glove.42B.300d.txt', load_used=True)
        #print("Done Loading Embeddings!")
        #print('Loaded {num} word vectors'.format(num=len(embeddings_index)))
        #print('Error encountered for {num} words'.format(num=errors))
        #print('Total Time Elapsed: {t:.1f} minutes'.format(t=(stop-start)/60.0))
        return w2i, word_emb_val

class TreeCreator:
    def __init__(self, noun_labels):
        self.noun_labels = noun_labels

    def get_nouns(self, tree, level, nouns):
        if((type(tree) == str) or (type(tree) == unicode)):
            return False
        for branch in tree:
            if(self.get_nouns(branch, level + 1, nouns) == False):
                if(tree.label() in self.noun_labels):
                    nouns.append([tree.leaves()[0], tree.label(), level])
        return True

    def get_noun_words(self, tree):
        nouns = []
        _ = self.get_nouns(tree, 0, nouns)
        sorted_nouns = list(sorted(nouns, key = lambda x:x[2]))
        return sorted_nouns

class TablePredictor:
    def __init__(self, embeddings_index, tree_creator, embeddings_dim, schema_content, level2_map, type_cats, parser):
        self.embeddings_index = embeddings_index
        self.tree_creator = tree_creator
        self.embeddings_dim = embeddings_dim
        self.level2_map = level2_map
        self.schema_content = schema_content
        self.type_cats = type_cats
        self.parser = parser

    def extract_table_info(self):
        info=self.schema_content.split('DROP TABLE IF EXISTS ')[1:]
        info=[x.split('CREATE TABLE ')[1] for x in info]
        info=[x.split('INSERT INTO')[0] for x in info]
        title_info=[re.findall(r'`[^`]*` ',x) for x in info]
        title_info = [x[0][1:-2] for x in title_info]
        content_info = [re.findall(r'`[^`]*` [^ ]*',x) for x in info]
        content_info = [x[1:] for x in content_info]
        content_info = [[y.split(' ') for y in x] for x in content_info]
        content_name = [[y[0][1:-1].replace('_', ' ') for y in x] for x in content_info]
        content_type = [[y[1] for y in x] for x in content_info]
        content_type = [[[z for z in self.type_cats if z in y] for y in x] for x in content_type]
        self.table_info=dict([(title_info[x], list(zip(content_name[x], content_type[x]))) for x in range(len(title_info))])
        self.tables=list(self.table_info.keys())

    def get_proximity(self, dbs):
        db_embeddings = {}
        for db in dbs:
            db_cols = [x[0] for x in self.table_info[db] if len(x[1])!=0]
            db_embeddings[db] = np.zeros((self.embeddings_dim, len(db_cols)), dtype = np.float32)
            non_err_ind = []
            for ind, col in enumerate(db_cols):
                col_emb = np.zeros((1, self.embeddings_dim), dtype = np.float32)
                col_word_cnt = 0
                for word in col.split():
                    if(word in self.embeddings_index[0]):
                        col_emb += w2emb(self.embeddings_index, word)
                        col_word_cnt += 1
                if(col_word_cnt != 0):
                    col_emb /= col_word_cnt
                    db_embeddings[db][:, ind] = col_emb
                    non_err_ind.append(ind)
            db_embeddings[db] = np.asarray(db_embeddings[db][:, non_err_ind])
        return db_embeddings

    def get_db_embeddings(self, category):
        firstWord_map = {}
        for table in self.tables:
            firstWord_map[table]=table.split('_')[0]
        cat_map=dict((x, self.level2_map[firstWord_map[x]]) for x in self.tables)
        category_dbs = [x for x in self.tables if cat_map[x] == category]
        self.db_embeddings = self.get_proximity(category_dbs)

    def predict_table(self, text, category):
        self.extract_table_info()
        self.get_db_embeddings(category)
        tree = self.parser.parse(text)
        nouns = self.tree_creator.get_noun_words(tree)

        noun_matrix = []
        noun_priority = []
        for ind, noun in enumerate(nouns):
            if(noun[0] in self.embeddings_index[0]):
                noun_matrix.append(w2emb(self.embeddings_index, noun[0]))
                noun_priority.append(noun[2])
        noun_matrix = np.asarray(noun_matrix, dtype = np.float32)
        noun_priority = np.asarray(noun_priority, dtype = np.float32)
        noun_priority = np.expand_dims(noun_priority, axis = 1)
        noun_priority = np.divide(noun_priority, noun_priority.sum())

        db_final_scores = {}
        db_max_scores = {}
        db_avg_scores = {}
        for db in self.db_embeddings:
            db_matrix = self.db_embeddings[db]
            db_scores = np.matmul(noun_matrix, db_matrix)
            noun_norm = np.linalg.norm(noun_matrix, ord = 2, axis = 1)
            noun_norm = np.expand_dims(noun_norm, axis = 1)
            noun_norm = np.repeat(noun_norm, db_scores.shape[1], axis = 1)
            db_norm = np.linalg.norm(db_matrix, ord = 2, axis = 0)
            db_norm = np.expand_dims(db_norm, axis = 0)
            db_norm = np.repeat(db_norm, db_scores.shape[0], axis = 0)
            db_scores = np.divide(db_scores, noun_norm)
            db_scores = np.divide(db_scores, db_norm)
            db_noun_priority = np.repeat(noun_priority, db_scores.shape[1], axis = 1)
            db_scores = np.multiply(db_scores, db_noun_priority)
            db_scores = np.sum(db_scores, axis = 0)
            db_final_scores[db] = db_scores
            db_max_scores[db] = np.max(db_scores) * 100
            db_avg_scores[db] = np.mean(db_scores) * 100

        sorted_max_dbs = list(sorted(db_max_scores.items(), key = lambda x:x[1], reverse = True))
        return [x[0] for x in sorted_max_dbs], [x[1] for x in sorted_max_dbs]

class Model:

    def __init__(self):

        schema_file=open('./schema.sql')
        schema_content=schema_file.read()

        level2_map={
            'owners':'owners',
            'apartments':'apartments',
            'payments':'payments',
        }

        type_cats = ['int', 'char', 'text', 'datetime', 'decimal']
        noun_labels = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS']
        parser=benepar.Parser("benepar_en2")

        embeddings_loader = EmbeddingsLoader('./glove/glove.42B.300d.txt')
        self.embeddings_dim = 300
        self.embeddings_index = embeddings_loader.load_embeddings()

        self.preprocessor = TextPreprocessor()
        self.plotter = Plotter(self.embeddings_index)

        cat_init = CategoryInitializer(self.embeddings_index, self.preprocessor, self.plotter, self.embeddings_dim, schema_content, level2_map)
        cat_init.extract_table_info()
        self.cat_descr_vector = cat_init.firstWord_mapping()

        tree_creator = TreeCreator(noun_labels)
        self.table_predictor = TablePredictor(self.embeddings_index, tree_creator, self.embeddings_dim, schema_content, level2_map, type_cats, parser)

    def predict(self, text, thresh = 60):
        
        category_predictor = CategoryPredictor(text, self.cat_descr_vector, self.embeddings_index, self.preprocessor, self.plotter, self.embeddings_dim)
        best_categories, best_cat_scores = category_predictor.predict_category(False)
        #print("\nPredicted Categories:")
        #for ind in range(3):
        #    print("{ind}. {cat}\n   ({perc:.1f}%)".format(ind = ind + 1, cat = best_categories[ind], perc = best_cat_scores[ind]))

        top5_tables = []
        for cat in best_categories:
            best_tables, best_table_scores = self.table_predictor.predict_table(text, cat)
            for table_ind in range(len(best_tables)):
                if(best_table_scores[table_ind] > thresh):
                    top5_tables.append((best_tables[table_ind], best_table_scores[table_ind], cat))
                    if(len(top5_tables) >= 5):
                        break
            if(len(top5_tables) >= 5):
                break

        #print("\nPredicted Tables:".format(cat = best_categories[0]))
        #for ind, table in enumerate(top5_tables):
        #    print("{ind}. {table} ({cat})\n   ({perc:.1f}%)".format(ind = ind + 1, table = table[0], cat = table[2], perc = table[1]))
        #if(len(top5_tables) == 0):
        #    print("No table predicted...")

        pred_tables = [x[0] for x in top5_tables]
        pred_tables_headers = [self.table_predictor.table_info[x] for x in pred_tables]
        pred_tables_headers = [[x[0] for x in y] for y in pred_tables_headers]

        return pred_tables, pred_tables_headers